{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq\n",
        "!pip install pyspark==3.5 delta-spark fastapi \"uvicorn[standard]\" -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya1kZS_BTBHP",
        "outputId": "90624643-b939-46fe-f6f6-ff40ab3e53ea",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 126333 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "from pyspark.sql.functions import to_date, col, current_timestamp\n",
        "from pyspark.sql.types import IntegerType, LongType\n",
        "\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"SmartFieldPoc\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "source_path = \"/content/predictive_maintenance_dataset.csv\"\n",
        "df = spark.read.csv(source_path, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "OM0E4mwQS5N-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Bronze Layer\n",
        "\n",
        "bronze_path = \"/tmp/delta/bronze_pmd\"\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(bronze_path)\n",
        "bronze_count = spark.read.format(\"delta\").load(bronze_path).count()\n",
        "print(f\"Bronze table written to {bronze_path} with {bronze_count} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goTzoLkvUn83",
        "outputId": "fd5e35de-1548-4d5d-e23a-f342137d2821"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bronze table written to /tmp/delta/bronze_pmd with 124494 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Silver Layer & Transformations\n",
        "\n",
        "silver_df = spark.read.format(\"delta\").load(bronze_path)\n",
        "# 1. Parse date and cast key columns to proper types\n",
        "silver_df = (\n",
        "    silver_df\n",
        "    .withColumn(\"event_date\", to_date(col(\"date\"), \"M/d/yyyy\"))\n",
        "    .withColumn(\"failure\", col(\"failure\").cast(IntegerType()))\n",
        "    .withColumn(\"metric1\", col(\"metric1\").cast(LongType()))\n",
        "    .withColumn(\"metric2\", col(\"metric2\").cast(IntegerType()))\n",
        "    .withColumn(\"metric3\", col(\"metric3\").cast(IntegerType()))\n",
        "    .withColumn(\"metric4\", col(\"metric4\").cast(IntegerType()))\n",
        "    .withColumn(\"metric5\", col(\"metric5\").cast(IntegerType()))\n",
        "    .withColumn(\"metric6\", col(\"metric6\").cast(LongType()))\n",
        "    .withColumn(\"metric7\", col(\"metric7\").cast(IntegerType()))\n",
        "    .withColumn(\"metric8\", col(\"metric8\").cast(IntegerType()))\n",
        "    .withColumn(\"metric9\", col(\"metric9\").cast(IntegerType()))\n",
        ")\n",
        "# 2. Identify & print duplicate records before dropping\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "dup_df = silver_df.groupBy(\"device\",\"event_date\").count().filter(\"count > 1\")\n",
        "dup_count = dup_df.count()\n",
        "if dup_count > 0:\n",
        "    print(f\"Found {dup_count} duplicate device-date entries:\")\n",
        "    dup_df.show(truncate=False)\n",
        "    # Show sample duplicate rows for inspection\n",
        "    silver_df.join(\n",
        "        dup_df.select(\"device\",\"event_date\"), [\"device\",\"event_date\"]\n",
        "    ).orderBy(\"device\",\"event_date\").show(truncate=False)\n",
        "else:\n",
        "    print(\"No duplicate device-date entries found.\")\n",
        "\n",
        "\n",
        "    # 3. Drop duplicates and remove rows with critical nulls\n",
        "silver_df = silver_df.dropDuplicates([\"device\",\"event_date\"]) \\\n",
        "    .filter(col(\"device\").isNotNull() & col(\"event_date\").isNotNull())\n",
        "\n",
        "# 4. Filter out unrealistic extreme values (e.g., negative metrics) Filter out unrealistic extreme values (e.g., negative metrics)\n",
        "silver_df = silver_df.filter(\n",
        "    (col(\"metric1\") >= 0) & (col(\"metric2\") >= 0) & (col(\"metric6\") >= 0)\n",
        ")\n",
        "# 5. Add metadata columns for auditing (e.g., ingest timestamp)\n",
        "silver_df = silver_df.withColumn(\"ingest_time\", current_timestamp())\n",
        "\n",
        "# 5. Write Silver table partitioned by event_date for performance & allow schema evolution\n",
        "silver_path = \"/tmp/delta/silver_pmd\"\n",
        "silver_df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .partitionBy(\"event_date\") \\\n",
        "    .save(silver_path)\n",
        "silver_count = spark.read.format(\"delta\").load(silver_path).count()\n",
        "print(f\"Silver table written with partitions to {silver_path} with {silver_count} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jhYVmOpcxVK",
        "outputId": "97b378da-7fde-4cb2-95eb-2b1c5ac5e8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 duplicate device-date entries:\n",
            "+--------+----------+-----+\n",
            "|device  |event_date|count|\n",
            "+--------+----------+-----+\n",
            "|S1F0R4Q8|2015-07-10|2    |\n",
            "+--------+----------+-----+\n",
            "\n",
            "+--------+----------+---------+-------+---------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "|device  |event_date|date     |failure|metric1  |metric2|metric3|metric4|metric5|metric6|metric7|metric8|metric9|\n",
            "+--------+----------+---------+-------+---------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "|S1F0R4Q8|2015-07-10|7/10/2015|0      |192721392|0      |0      |0      |8      |213700 |0      |0      |0      |\n",
            "|S1F0R4Q8|2015-07-10|7/10/2015|0      |192721392|0      |0      |0      |8      |213700 |0      |0      |0      |\n",
            "+--------+----------+---------+-------+---------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "\n",
            "Silver table written with partitions to /tmp/delta/silver_pmd with 124493 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Data Validation & Quality Checks\n",
        "\n",
        "from pyspark.sql.functions import count, when, col\n",
        "\n",
        "silver = spark.read.format(\"delta\").load(silver_path)\n",
        "total_silver = silver.count()\n",
        "print(f\"Total rows in Silver: {total_silver}\")\n",
        "\n",
        "# 6.0 Bronze vs Silver row count validation\n",
        "if bronze_count == total_silver:\n",
        "    print(\"All bronze rows have been processed into Silver.\")\n",
        "else:\n",
        "    diff = bronze_count - total_silver\n",
        "    print(f\"Warning: Bronze vs Silver row count mismatch. Bronze has {bronze_count} rows; Silver has {total_silver} rows.\")\n",
        "    print(f\"Total rows dropped or filtered during transformation: {diff}\")\n",
        "\n",
        "# 6.1 Null checks for critical columns\n",
        "null_counts = silver.select(\n",
        "    *[count(when(col(c).isNull(), True)).alias(c + '_nulls')\n",
        "      for c in ['device','event_date','failure','metric1','metric2','metric6']]\n",
        ")\n",
        "null_counts.show()\n",
        "assert all(v == 0 for v in null_counts.collect()[0]), \"Null values found in critical columns\"\n",
        "print(\"No nulls in critical columns.\")\n",
        "\n",
        "# 6.2 Duplicate check: ensure each device-date pair is unique\n",
        "dup_count = silver.groupBy(\"device\",\"event_date\").count().filter(\"count > 1\").count()\n",
        "assert dup_count == 0, f\"Found {dup_count} duplicate device-date entries\"\n",
        "print(\"No duplicate device-date records.\")\n",
        "\n",
        "# 6.3 Range checks for metrics: negative values indicate errors\n",
        "range_issues = silver.select(\n",
        "    count(when(col('metric1') < 0, True)).alias('metric1_neg'),\n",
        "    count(when(col('metric2') < 0, True)).alias('metric2_neg'),\n",
        "    count(when(col('metric6') < 0, True)).alias('metric6_neg')\n",
        ")\n",
        "range_issues.show()\n",
        "assert range_issues.collect()[0]['metric1_neg'] == 0, \"Negative values found in metric1\"\n",
        "print(\"No negative metric values.\")\n",
        "\n",
        "# 6.4 Failure flag sanity: values should only be 0 or 1\n",
        "invalid_fail = silver.filter(~col('failure').isin([0,1])).count()\n",
        "assert invalid_fail == 0, f\"Invalid failure flags: {invalid_fail}\"\n",
        "print(\"Failure flags are valid (0 or 1).\")\n",
        "\n",
        "print(\"Data quality validation completed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efvq8UTTg1Oc",
        "outputId": "3306daf7-6385-4074-e125-c915ee9dca86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in Silver: 124493\n",
            "Warning: Bronze vs Silver row count mismatch. Bronze has 124494 rows; Silver has 124493 rows.\n",
            "Total rows dropped or filtered during transformation: 1\n",
            "+------------+----------------+-------------+-------------+-------------+-------------+\n",
            "|device_nulls|event_date_nulls|failure_nulls|metric1_nulls|metric2_nulls|metric6_nulls|\n",
            "+------------+----------------+-------------+-------------+-------------+-------------+\n",
            "|           0|               0|            0|            0|            0|            0|\n",
            "+------------+----------------+-------------+-------------+-------------+-------------+\n",
            "\n",
            "No nulls in critical columns.\n",
            "No duplicate device-date records.\n",
            "+-----------+-----------+-----------+\n",
            "|metric1_neg|metric2_neg|metric6_neg|\n",
            "+-----------+-----------+-----------+\n",
            "|          0|          0|          0|\n",
            "+-----------+-----------+-----------+\n",
            "\n",
            "No negative metric values.\n",
            "Failure flags are valid (0 or 1).\n",
            "Metric1 p1: 0.0, p99: 244140480.0\n",
            "Metric2 p1: 0.0, p99: 64968.0\n",
            "Data quality validation completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature Engineering\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg, max, dayofmonth, month, lag, col\n",
        "\n",
        "# Time-series features: rolling avg of metric1 over past 3 days\n",
        "window_spec = Window.partitionBy(\"device\").orderBy(\"event_date\").rowsBetween(-3, 0)\n",
        "silver_ts = silver_df.withColumn(\"rolling_avg_metric1_3d\", avg(\"metric1\").over(window_spec))\n",
        "# Lag feature: previous day's metric2\n",
        "silver_ts = silver_ts.withColumn(\"lag_metric2_1d\", lag(\"metric2\", 1).over(Window.partitionBy(\"device\").orderBy(\"event_date\")))\n",
        "\n",
        "# Categorical features: extract month and day\n",
        "silver_ts = silver_ts.withColumn(\"event_month\", month(col(\"event_date\"))) \\\n",
        "                   .withColumn(\"event_day\", dayofmonth(col(\"event_date\")))\n",
        "\n",
        "# Aggregate to feature table\n",
        "features_df = (\n",
        "    silver_ts\n",
        "    .groupBy(\"device\")\n",
        "    .agg(\n",
        "        avg(\"rolling_avg_metric1_3d\").alias(\"avg_roll3d_metric1\"),\n",
        "        avg(\"lag_metric2_1d\").alias(\"avg_lag1d_metric2\"),\n",
        "        avg(\"metric1\").alias(\"avg_metric1\"),\n",
        "        max(\"metric2\").alias(\"max_metric2\")\n",
        "    )\n",
        ")\n",
        "# Join back a categorical example\n",
        "features_df = features_df.join(\n",
        "    silver_ts.select(\"device\",\"event_month\",\"event_day\").distinct(), on=\"device\", how=\"left\"\n",
        ")\n",
        "\n",
        "features_df.show(5)\n",
        "features_path = \"/tmp/delta/features_pmd\"\n",
        "features_df.write.format(\"delta\").mode(\"overwrite\").save(features_path)\n",
        "print(f\"Feature table written to {features_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r84wLZWDhjJg",
        "outputId": "fce95457-801e-4115-882b-4d9b04a781e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+-----------------+--------------------+-----------+-----------+---------+\n",
            "|  device|  avg_roll3d_metric1|avg_lag1d_metric2|         avg_metric1|max_metric2|event_month|event_day|\n",
            "+--------+--------------------+-----------------+--------------------+-----------+-----------+---------+\n",
            "|S1F01085|1.2712392355555554E8|             55.8|1.1593295066666667E8|         56|          1|        3|\n",
            "|S1F01085|1.2712392355555554E8|             55.8|1.1593295066666667E8|         56|          1|        1|\n",
            "|S1F01085|1.2712392355555554E8|             55.8|1.1593295066666667E8|         56|          1|        2|\n",
            "|S1F01085|1.2712392355555554E8|             55.8|1.1593295066666667E8|         56|          1|        4|\n",
            "|S1F01085|1.2712392355555554E8|             55.8|1.1593295066666667E8|         56|          1|        5|\n",
            "+--------+--------------------+-----------------+--------------------+-----------+-----------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Feature table written to /tmp/delta/features_pmd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sample API to ML\n",
        "\n",
        "pandas_df = features_df.toPandas()\n",
        "print(\"Converted features to Pandas DataFrame:\", pandas_df.shape)\n",
        "\n",
        "# Convert pandas data to dict for API\n",
        "feature_dict = pandas_df.to_dict(orient='list')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD9qJlbciABI",
        "outputId": "fda9047e-e66a-4351-bca9-629f50183718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted features to Pandas DataFrame: (124493, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_code = f'''\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import pandas as pd\n",
        "\n",
        "# Load features into memory\n",
        "features = pd.DataFrame({feature_dict})\n",
        "\n",
        "app = FastAPI(title=\"Predictive Maintenance Feature API\", description=\"Serves features for ML models\", version=\"1.0\")\n",
        "\n",
        "class FeatureResponse(BaseModel):\n",
        "    device: str\n",
        "    avg_roll3d_metric1: float\n",
        "    avg_lag1d_metric2: float\n",
        "    avg_metric1: float\n",
        "    max_metric2: float\n",
        "    event_month: int\n",
        "    event_day: int\n",
        "\n",
        "@app.get(\"/features/{{device}}\", response_model=FeatureResponse, tags=[\"Features\"])\n",
        "def get_features(device: str):\n",
        "    df = features[features[\"device\"] == device]\n",
        "    if df.empty:\n",
        "        raise HTTPException(status_code=404, detail=\"Device not found\")\n",
        "    record = df.iloc[0].to_dict()\n",
        "    return record\n",
        "\n",
        "# Run with: uvicorn app:app --reload --host 0.0.0.0 --port 8000\n",
        "'''\n",
        "with open(\"app.py\",\"w\") as f:\n",
        "    f.write(api_code)\n",
        "print(\"FastAPI app created: app.py (Swagger available at /docs)\")\n",
        "\n",
        "# Cell 10: (Optional) Start API server\n",
        "# !uvicorn app:app --reload --host 0.0.0.0 --port 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFbxDx_liCCs",
        "outputId": "8d8bda93-b4ec-420d-956b-98d85de5a4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI app created: app.py (Swagger available at /docs)\n"
          ]
        }
      ]
    }
  ]
}